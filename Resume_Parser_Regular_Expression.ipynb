{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Resume-Parser_Regular-Expression.ipynb",
      "provenance": [],
      "mount_file_id": "1AtiajdHJKbKcfsPpoYbgrrVKaUZtrGrZ",
      "authorship_tag": "ABX9TyNQiYqFdRiNpT4ksduCAhVO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deepankar-pathak1/Resume-Analyzer/blob/main/Resume_Parser_Regular_Expression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7V9GFLk0-et"
      },
      "source": [
        "# # Resume Parser using Regular Expression\r\n",
        "# !pip install pdfminer.six\r\n",
        "# https://promptapi.com/blog/article/build-your-own-resume-parser-using-python-and-nlp\r\n",
        "import nltk"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMgXKE521usB",
        "outputId": "cf4b01cb-ae42-4e67-d5dd-8911fb3afb9f"
      },
      "source": [
        "### Text From Pdf \r\n",
        "from pdfminer.high_level import extract_text\r\n",
        "\r\n",
        "def extract_text_from_pdf(pdf_path):\r\n",
        "    return extract_text(pdf_path)\r\n",
        "\r\n",
        "if __name__ == '__main__':\r\n",
        "    print(extract_text_from_pdf(r'/content/Alice Clark CV.pdf'))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "•  20+ years of experience in data handling, design, and development \n",
            "•  Data Warehouse: Data analysis, star/snow flake scema data modelling and design specific to \n",
            "\n",
            "data warehousing and business intelligence \n",
            "\n",
            "•  Database: Experience in database designing, scalability, back-up and recovery, writing and \n",
            "\n",
            "optimizing SQL code and Stored Procedures, creating functions, views, triggers and indexes. \n",
            "Cloud platform: Worked on Microsoft Azure cloud services like Document DB, SQL Azure, \n",
            "Stream Analytics, Event hub, Power BI, Web Job, Web App, Power BI, Azure data lake \n",
            "analytics(U-SQL) \n",
            "\n",
            "Alice Clark \n",
            "AI / Machine Learning \n",
            " \n",
            "\n",
            "Delhi, India Email me on Indeed \n",
            "\n",
            "Willing to relocate anywhere \n",
            "\n",
            "WORK EXPERIENCE \n",
            "\n",
            "Software Engineer \n",
            "\n",
            "Microsoft – Bangalore, Karnataka \n",
            "\n",
            "January 2000 to Present \n",
            "\n",
            "1. Microsoft Rewards Live dashboards: \n",
            "\n",
            "Description: - Microsoft rewards is loyalty program that rewards Users for browsing and shopping \n",
            "online. Microsoft Rewards members can earn points when searching with Bing, browsing with \n",
            "Microsoft Edge and making purchases at the Xbox Store, the Windows Store and the Microsoft \n",
            "Store. Plus, user can pick up bonus points for taking daily quizzes and tours on the Microsoft \n",
            "rewards website. Rewards live dashboards gives a live picture of usage world-wide and by \n",
            "markets like US, Canada, Australia, new user registration count, top/bottom performing rewards \n",
            "offers, orders stats and weekly trends of user activities, orders and new user registrations. the \n",
            "PBI tiles gets refreshed in different frequencies starting from 5 seconds to 30 minutes. \n",
            "Technology/Tools used \n",
            " \n",
            "\n",
            "Indian Institute of Technology – Mumbai \n",
            "\n",
            "EDUCATION \n",
            "\n",
            "2001 \n",
            "\n",
            "SKILLS \n",
            "\n",
            "Machine Learning, Natural Language Processing, and Big Data Handling \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            " \n",
            "\n",
            "\fADDITIONAL INFORMATION \n",
            "\n",
            "Professional Skills \n",
            "\n",
            "• Excellent analytical, problem solving, communication, knowledge transfer and interpersonal \n",
            "skills with ability to interact with individuals at all the levels \n",
            "• Quick learner and maintains cordial relationship with project manager and team members and \n",
            "good performer both in team and independent job environments \n",
            "• Positive attitude towards superiors &amp; peers \n",
            "• Supervised junior developers throughout project lifecycle and provided technical assistance \n",
            "\n",
            "\f\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfUXgqRx2LU4"
      },
      "source": [
        "# pip install docx2txt\r\n",
        "# # example_02.py\r\n",
        "\r\n",
        "# import docx2txt\r\n",
        "\r\n",
        "\r\n",
        "# def extract_text_from_docx(docx_path):\r\n",
        "#     txt = docx2txt.process(docx_path)\r\n",
        "#     if txt:\r\n",
        "#         return txt.replace('\\t', ' ')\r\n",
        "#     return None\r\n",
        "\r\n",
        "\r\n",
        "# if __name__ == '__main__':\r\n",
        "#     print(extract_text_from_docx('./resume.docx'))  # noqa: T001"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWKl_63a2a1h",
        "outputId": "b15077c5-049e-418b-da6e-c3490ab9bec8"
      },
      "source": [
        "!pip install nltk\r\n",
        "!pip install numpy "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvpavLYt2jdE"
      },
      "source": [
        "# # example_04.py\r\n",
        "\r\n",
        "# # import docx2txt\r\n",
        "# import nltk\r\n",
        "\r\n",
        "# nltk.download('punkt')\r\n",
        "# nltk.download('averaged_perceptron_tagger')\r\n",
        "# nltk.download('maxent_ne_chunker')\r\n",
        "# nltk.download('words')\r\n",
        "\r\n",
        "\r\n",
        "# def extract_text_from_pdf(pdf_path):\r\n",
        "#     txt =  extract_text(pdf_path)\r\n",
        "#     if txt:\r\n",
        "#         return txt.replace('\\t', ' ')\r\n",
        "#     return None\r\n",
        "\r\n",
        "\r\n",
        "# def extract_names(txt):\r\n",
        "#     person_names = []\r\n",
        "\r\n",
        "#     for sent in nltk.sent_tokenize(txt):\r\n",
        "#         for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\r\n",
        "#             if hasattr(chunk, 'label') and chunk.label() == 'PERSON':\r\n",
        "#                 person_names.append(\r\n",
        "#                     ' '.join(chunk_leave[0] for chunk_leave in chunk.leaves())\r\n",
        "#                 )\r\n",
        "\r\n",
        "#     return person_names\r\n",
        "\r\n",
        "\r\n",
        "# if __name__ == '__main__':\r\n",
        "#     text = extract_text_from_pdf(r'/content/Alice Clark CV.pdf')\r\n",
        "#     names = extract_names(text)\r\n",
        "\r\n",
        "#     if names:\r\n",
        "#         print(names[0])  # noqa: T001"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnulgggy6oHc",
        "outputId": "8cf90cf0-2935-4541-f294-2eaaa3292a8e"
      },
      "source": [
        "import spacy                                                                                                                            \r\n",
        "txt =  extract_text(r'/content/Alice Clark CV.pdf')\r\n",
        "nlp = spacy.load('en')                                                                                                                  \r\n",
        "sents = nlp(txt) \r\n",
        "people = [ee for ee in sents.ents if ee.label_ == 'PERSON']\r\n",
        "\r\n",
        "print(people)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Stored Procedures, Cloud, Document DB, Web Job, Web App, Alice Clark, Bing]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b18y383W7pdZ",
        "outputId": "9c39f418-1be6-4a56-9cfd-1a878255d254"
      },
      "source": [
        "!pip install dateparser"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting dateparser\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/78/c4/b5ddc3eeac974d85055d88c1e6b62cc492fc1a93dbe3b66a45a756a7b807/dateparser-1.0.0-py2.py3-none-any.whl (279kB)\n",
            "\u001b[K     |████████████████████████████████| 286kB 5.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from dateparser) (2018.9)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from dateparser) (2.8.1)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.6/dist-packages (from dateparser) (1.5.1)\n",
            "Requirement already satisfied: regex!=2019.02.19 in /usr/local/lib/python3.6/dist-packages (from dateparser) (2019.12.20)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil->dateparser) (1.15.0)\n",
            "Installing collected packages: dateparser\n",
            "Successfully installed dateparser-1.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "464V1pEE7hOD"
      },
      "source": [
        "# from spacy.tokens import Span\r\n",
        "# import dateparser\r\n",
        "\r\n",
        "# def expand_person_entities(doc):\r\n",
        "#     new_ents = []\r\n",
        "#     for ent in doc.ents:\r\n",
        "#         # Only check for title if it's a person and not the first token\r\n",
        "#         if ent.label_ == \"PERSON\":\r\n",
        "#             if ent.start != 0:\r\n",
        "#                 # if person preceded by title, include title in entity\r\n",
        "#                 prev_token = doc[ent.start - 1]\r\n",
        "#                 if prev_token.text in (\"Dr\", \"Dr.\", \"Mr\", \"Mr.\", \"Ms\", \"Ms.\"):\r\n",
        "#                     new_ent = Span(doc, ent.start - 1, ent.end, label=ent.label)\r\n",
        "#                     new_ents.append(new_ent)\r\n",
        "#                 else:\r\n",
        "#                     # if entity can be parsed as a date, it's not a person\r\n",
        "#                     if dateparser.parse(ent.text) is None:\r\n",
        "#                         new_ents.append(ent) \r\n",
        "#         else:\r\n",
        "#             new_ents.append(ent)\r\n",
        "#     doc.ents = new_ents\r\n",
        "#     return doc\r\n",
        "\r\n",
        "# nlp.add_pipe(expand_person_entities, after='ner')\r\n",
        "# sents = nlp(txt) \r\n",
        "# [(ent.text, ent.label_) for ent in sents.ents if ent.label_=='PERSON']\r\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJfzQPy93iIv",
        "outputId": "23e9ef99-5e05-4010-9cfd-dab3372cc3d6"
      },
      "source": [
        "# example_05.py\r\n",
        "\r\n",
        "import re\r\n",
        "import subprocess  # noqa: S404\r\n",
        "\r\n",
        "PHONE_REG = re.compile(r'[\\+\\(]?[1-9][0-9 .\\-\\(\\)]{8,}[0-9]')\r\n",
        "\r\n",
        "\r\n",
        "def extract_text_from_pdf(pdf_path):\r\n",
        "    txt =  extract_text(pdf_path)\r\n",
        "    if txt:\r\n",
        "        return txt.replace('\\t', ' ')\r\n",
        "    return None\r\n",
        "\r\n",
        "\r\n",
        "def extract_phone_number(resume_text):\r\n",
        "    phone = re.findall(PHONE_REG, resume_text)\r\n",
        "\r\n",
        "    if phone:\r\n",
        "        number = ''.join(phone[0])\r\n",
        "\r\n",
        "        if resume_text.find(number) >= 0 and len(number) < 16:\r\n",
        "            return number\r\n",
        "    return None\r\n",
        "\r\n",
        "\r\n",
        "if __name__ == '__main__':\r\n",
        "    text = extract_text_from_pdf(r'/content/Deepankar Pathak Data Scientist..- (1).pdf')\r\n",
        "    phone_number = extract_phone_number(text)\r\n",
        "\r\n",
        "    print(phone_number)  # noqa: T001"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9166759124\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qans1z8d4NcA",
        "outputId": "34f42132-6ab6-449f-94bb-a02f4c8552df"
      },
      "source": [
        "# example_07.py\r\n",
        "\r\n",
        "\r\n",
        "import nltk\r\n",
        "\r\n",
        "nltk.download('stopwords')\r\n",
        "\r\n",
        "# you may read the database from a csv file or some other database\r\n",
        "SKILLS_DB = [\r\n",
        "    'machine learning',\r\n",
        "    'data science',\r\n",
        "    'python',\r\n",
        "    'word',\r\n",
        "    'excel',\r\n",
        "    'English',\r\n",
        "]\r\n",
        "\r\n",
        "\r\n",
        "def extract_text_from_pdf(pdf_path):\r\n",
        "    txt =  extract_text(pdf_path)\r\n",
        "    if txt:\r\n",
        "        return txt.replace('\\t', ' ')\r\n",
        "    return None\r\n",
        "\r\n",
        "\r\n",
        "def extract_skills(input_text):\r\n",
        "    stop_words = set(nltk.corpus.stopwords.words('english'))\r\n",
        "    word_tokens = nltk.tokenize.word_tokenize(input_text)\r\n",
        "\r\n",
        "    # remove the stop words\r\n",
        "    filtered_tokens = [w for w in word_tokens if w not in stop_words]\r\n",
        "\r\n",
        "    # remove the punctuation\r\n",
        "    filtered_tokens = [w for w in word_tokens if w.isalpha()]\r\n",
        "\r\n",
        "    # generate bigrams and trigrams (such as artificial intelligence)\r\n",
        "    bigrams_trigrams = list(map(' '.join, nltk.everygrams(filtered_tokens, 2, 3)))\r\n",
        "\r\n",
        "    # we create a set to keep the results in.\r\n",
        "    found_skills = set()\r\n",
        "\r\n",
        "    # we search for each token in our skills database\r\n",
        "    for token in filtered_tokens:\r\n",
        "        if token.lower() in SKILLS_DB:\r\n",
        "            found_skills.add(token)\r\n",
        "\r\n",
        "    # we search for each bigram and trigram in our skills database\r\n",
        "    for ngram in bigrams_trigrams:\r\n",
        "        if ngram.lower() in SKILLS_DB:\r\n",
        "            found_skills.add(ngram)\r\n",
        "\r\n",
        "    return found_skills\r\n",
        "\r\n",
        "\r\n",
        "if __name__ == '__main__':\r\n",
        "    text = extract_text_from_pdf(r'/content/Deepankar Pathak Data Scientist..- (1).pdf')\r\n",
        "    skills = extract_skills(text)\r\n",
        "\r\n",
        "    print(skills)  # noqa: T001"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "{'excel', 'Python', 'Machine Learning', 'python', 'Data Science'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tKtEk2T9ath"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}